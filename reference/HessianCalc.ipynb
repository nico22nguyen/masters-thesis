{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from resnet import ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "\ttransforms.RandomCrop(32, padding=4),\n",
    "\ttransforms.RandomHorizontalFlip(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "trainset_nocrop_noflip = CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
    "trainloader_for_Jacobian  = DataLoader(trainset_nocrop_noflip, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "sampler=np.random.choice(np.arange(0, 50000), 45000, replace=False)\n",
    "trainloader_RandomReduced = DataLoader(trainset, batch_size=128, sampler=sampler, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet34()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "\tnet = torch.nn.DataParallel(net)\n",
    "\tcudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
      "           Conv2d-15           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-17           [-1, 64, 32, 32]               0\n",
      "           Conv2d-18          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-19          [-1, 128, 16, 16]             256\n",
      "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "           Conv2d-22          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
      "           Conv2d-25          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-26          [-1, 128, 16, 16]             256\n",
      "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-29          [-1, 128, 16, 16]               0\n",
      "           Conv2d-30          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
      "           Conv2d-32          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-33          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-34          [-1, 128, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-39          [-1, 128, 16, 16]               0\n",
      "           Conv2d-40            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "           Conv2d-42            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "           Conv2d-44            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "           Conv2d-49            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-50            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-51            [-1, 256, 8, 8]               0\n",
      "           Conv2d-52            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-53            [-1, 256, 8, 8]             512\n",
      "           Conv2d-54            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-55            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-56            [-1, 256, 8, 8]               0\n",
      "           Conv2d-57            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-58            [-1, 256, 8, 8]             512\n",
      "           Conv2d-59            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-60            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-61            [-1, 256, 8, 8]               0\n",
      "           Conv2d-62            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-63            [-1, 256, 8, 8]             512\n",
      "           Conv2d-64            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-65            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-66            [-1, 256, 8, 8]               0\n",
      "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
      "           Conv2d-69            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-70            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-71            [-1, 256, 8, 8]               0\n",
      "           Conv2d-72            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-73            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-74            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-75            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-76            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-77            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-80            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-81            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-82            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-83            [-1, 512, 4, 4]               0\n",
      "           Conv2d-84            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-85            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-86            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-87            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-88            [-1, 512, 4, 4]               0\n",
      "           Linear-89                   [-1, 10]           5,130\n",
      "           ResNet-90                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 21,282,122\n",
      "Trainable params: 21,282,122\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.06\n",
      "Params size (MB): 81.18\n",
      "Estimated Total Size (MB): 100.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('==> Resuming from checkpoint..')\n",
    "# #assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "# checkpoint = torch.load('./ckpt_afterfix_50epoch_0.001reg.pth')\n",
    "# net.load_state_dict(checkpoint['net'])\n",
    "# best_acc = checkpoint['acc']\n",
    "# start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CosineAnnealingLR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m scheduler1  \u001b[38;5;241m=\u001b[39m \u001b[43mCosineAnnealingLR\u001b[49m(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[0;32m      4\u001b[0m scheduler2 \u001b[38;5;241m=\u001b[39m LinearLR(optimizer, start_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, end_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, total_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m)\n\u001b[0;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m SequentialLR(optimizer, schedulers\u001b[38;5;241m=\u001b[39m[scheduler1, scheduler2], milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2500\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CosineAnnealingLR' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler1  = CosineAnnealingLR(optimizer, T_max=20000)\n",
    "scheduler2 = LinearLR(optimizer, start_factor=0.0001, end_factor=1.0, total_iters=2500)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[2500])\n",
    "\n",
    "def train(epoch):\n",
    "\tprint('\\nEpoch: %d' % epoch)\n",
    "\tnet.train()\n",
    "\ttrain_loss = 0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\tno_of_training_steps = 0\n",
    "\tfor batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "\t\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = net(inputs)\n",
    "\t\t\tloss = criterion(outputs, targets)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\t\t_, predicted = outputs.max(1)\n",
    "\t\t\ttotal += targets.size(0)\n",
    "\t\t\tcorrect += predicted.eq(targets).sum().item()\n",
    "\t\t\tno_of_training_steps = no_of_training_steps + 1\n",
    "\n",
    "\t\t\tscheduler.step()\n",
    "\tprint(f'Epoch {epoch + 1} -- accuracy: {correct/total:.5f}, num_correct: {correct}, total: {total}, training steps: {no_of_training_steps}')\n",
    "\t#progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'  % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\treturn 100.*correct/total\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "\n",
    "    print('test',100.*correct/total, correct, total)\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8968e9e73aa747109656003e0efd992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train 23.792 11896 50000 391\n",
      "\n",
      "Epoch: 1\n",
      "train 35.458 17729 50000 391\n",
      "\n",
      "Epoch: 2\n",
      "train 44.132 22066 50000 391\n",
      "\n",
      "Epoch: 3\n",
      "train 51.96 25980 50000 391\n",
      "\n",
      "Epoch: 4\n",
      "train 57.858 28929 50000 391\n",
      "\n",
      "Epoch: 5\n",
      "train 63.286 31643 50000 391\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nico\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 69.37 34685 50000 391\n",
      "\n",
      "Epoch: 7\n",
      "train 72.552 36276 50000 391\n",
      "\n",
      "Epoch: 8\n",
      "train 72.594 36297 50000 391\n",
      "\n",
      "Epoch: 9\n",
      "train 72.86 36430 50000 391\n",
      "\n",
      "Epoch: 10\n",
      "train 73.632 36816 50000 391\n",
      "\n",
      "Epoch: 11\n",
      "train 75.07 37535 50000 391\n",
      "\n",
      "Epoch: 12\n",
      "train 76.838 38419 50000 391\n",
      "\n",
      "Epoch: 13\n",
      "train 79.346 39673 50000 391\n",
      "\n",
      "Epoch: 14\n",
      "train 81.074 40537 50000 391\n",
      "\n",
      "Epoch: 15\n",
      "train 82.526 41263 50000 391\n",
      "\n",
      "Epoch: 16\n",
      "train 83.814 41907 50000 391\n",
      "\n",
      "Epoch: 17\n",
      "train 85.124 42562 50000 391\n",
      "\n",
      "Epoch: 18\n",
      "train 85.836 42918 50000 391\n",
      "\n",
      "Epoch: 19\n",
      "train 86.72 43360 50000 391\n",
      "\n",
      "Epoch: 20\n",
      "train 87.51 43755 50000 391\n",
      "\n",
      "Epoch: 21\n",
      "train 88.034 44017 50000 391\n",
      "\n",
      "Epoch: 22\n",
      "train 88.652 44326 50000 391\n",
      "\n",
      "Epoch: 23\n",
      "train 89.274 44637 50000 391\n",
      "\n",
      "Epoch: 24\n",
      "train 89.728 44864 50000 391\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "\ttrain(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(25, 30)):\n",
    "\ttrain(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 84.71 8471 10000\n",
      "Saving..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.71"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JacobianList = []\n",
    "TargetList = []\n",
    "for inputs, labels in tqdm(trainloader_for_Jacobian):\n",
    "  TargetList.append(labels)\n",
    "  start = time()\n",
    "  jacobian = torch.autograd.functional.jacobian(lambda x: criterion(net(x), labels.to(device)), inputs.to(device))\n",
    "  JacobianList.append(np.linalg.norm(jacobian.to(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f5f5d2e5448399a53675883fa022a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HessianNormList = []\n",
    "# TargetList = []\n",
    "# idx_list = []\n",
    "# for i, (inputs, labels) in tqdm(enumerate(trainloader_forHessian)):\n",
    "#   idx_list.append(i)\n",
    "#   TargetList.append(labels)\n",
    "#   start = time()\n",
    "#   hessian_comp = hessian(net, criterion, data=(inputs, labels))\n",
    "#   top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=1)\n",
    "#   #print(\"pyhessian : \" , top_eigenvalues  , \"time : \", (time() - start)         , \" seconds \"  )\n",
    "#   HessianNormList.append( top_eigenvalues )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(JacobianList).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'JacobianNormList_0.005' #'WideResNet-70-16'\n",
    "fileObject = open(name, 'wb')\n",
    "pickle.dump(JacobianList, fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'TargetList_0.005' #'WideResNet-70-16'\n",
    "fileObject = open(name, 'wb')\n",
    "pickle.dump(np.array([np.array(x) for x in TargetList]), fileObject)\n",
    "fileObject.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
